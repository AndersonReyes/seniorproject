{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import hamming_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will not be needing the id column so delete it, and get the labels for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "del train['id']\n",
    "labels = train[categories].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the data safe comments are those for which all the category columns are 0 but to make it more readable we will add another 'safe' column with a 1 when all the other columns are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our vocabulary\n",
    "\n",
    "* Split each sentence into tokens where a token is a word in the sentence  \n",
    "* Use nltk to get the frequency of each word in the corpus  \n",
    "* Get the `vocabSize` most common words  \n",
    "* encode words in integers were i is the ith word in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 20000 # FOR NOW USE ALL OF THE VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it in a function for future testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, maxSentLength=500):\n",
    "    sentences = df[['comment_text']].values.tolist()\n",
    "    tokenizedSentences = [sent[0].split(' ') for sent in sentences]\n",
    "    wordFrequencies = nltk.FreqDist(itertools.chain(*tokenizedSentences))\n",
    "    vocab = wordFrequencies.most_common(vocabSize-1)\n",
    "    indexToWord = [word[0] for word in vocab]\n",
    "    indexToWord = ['unkown'] + indexToWord\n",
    "    wordToIndex = dict([(w, i) for i, w in enumerate(indexToWord)])\n",
    "    for i, sent in enumerate(tokenizedSentences):\n",
    "        tokenizedSentences[i] = [wordToIndex[w]  if w in wordToIndex else 0 for w in sent]\n",
    "    \n",
    "    tokenizedSentences = sequence.pad_sequences(tokenizedSentences, maxlen=maxSentLength)\n",
    "    preprocessedValues = {\n",
    "        'sentences': sentences,\n",
    "        'tokenizedSentences': tokenizedSentences,\n",
    "        'wordFrequencies': wordFrequencies,\n",
    "        'vocab': vocab,\n",
    "        'indexToWord': indexToWord,\n",
    "        'wordToIndex': wordToIndex\n",
    "    }\n",
    "    \n",
    "    return preprocessedValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a reduction of the problem we will only use safe or not safe (reduced it to a binary classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters and data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will also notice that we cut the sentences to 500 words as another preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSentLength=200\n",
    "processedData = preprocess(train, maxSentLength=maxSentLength)\n",
    "tokenizedSentences = processedData['tokenizedSentences']\n",
    "N = len(tokenizedSentences)\n",
    "split = int(.80 * N) # use 80% for training\n",
    "embeddingSize = 128\n",
    "\n",
    "\n",
    "xTrain = tokenizedSentences[:split]\n",
    "xTest = tokenizedSentences[split:]\n",
    "\n",
    "yTrain = np.array(labels[:split])\n",
    "yTest = np.array(labels[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(categories)\n",
    "lstmModel = Sequential()\n",
    "lstmModel.add(Embedding(vocabSize, embeddingSize, input_length=maxSentLength))\n",
    "lstmModel.add(LSTM(200))\n",
    "lstmModel.add(Dense(C, activation='sigmoid'))\n",
    "lstmModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstmModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmModel.fit(xTrain, yTrain, validation_data=(xTest, yTest), batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = lstmModel.to_json()\n",
    "with open('./models/lstm_final.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "lstmModel.save_weights('./models/lstm_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.evaluate(xTest, yTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
